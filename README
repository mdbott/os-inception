OS-Inception: you have to go deeper!
=========

OS-Inception is a set of Ansible 2 roles and playbooks that assist in setting up a Red Hat OpenStack Platform Cloud on an existing Overcloud, including a set of 3 Red Hat Ceph Storage clustered servers providing storage in the virtual environment.

This is ideal for test environments or where developers need to work on the underlying OpenStack infrastructure, but the work in greatly enhanced when the environments are separated so that a broken component in one developer's lab doesn't break everyone else's development environment. It truly implements an OpenStack on OpenStack set of systems. Also, note that these playbooks make full use of Composeable roles to split out Pacemaker and SystemD Controller nodes. This was done to make troubleshooting easier by making the traffic between these services easier to track, however you may choose to recombine these into a single controller node by modifying the playbooks.

Once the run of the playbook is completed, the end-state will be a separate tenant project with its own networks, an inception instance (which holds these playbooks), an OpenStack Director instance with a set of templates ready for modification and to deploy, and a set of blank instances brought into the cluster using a prebuilt pxe_boot image to simulate the deployment of the PXE boot so that the OS-Inception resources can be spread across all compute nodes in your underlying OSP system. The ipmitool binary is replaced with a script taken from hextupleO (https://github.com/OOsemka/hextupleo) which allows IPMI commands to be passed to the underlying base OSP Nova system in order to emulate power-on,d power-off, and power-status commands, meaning pxe_ipmitool driver works - no pxe_ssh required. This also works if your Inception overcloud is spread across multiple Compute nodes for the baremetal OpenStack environment.

In order to run multiple Inception nodes, the inception hosts will live all in a single development project.  Each inception node will create its own project based on the environment variables used in the playbook, and all other hosts will be created in the unique project.

There are some prerequisites in order for this to execute correctly, which are detailed below.

Note: "BASTION" and "Inception" are interchangeable when referring to the inception instance

Requirements
==========
In order for OS-Inception to work you will need an existing OpenStack cluster with sufficient resources to run 13 VM's:
+-----------------------+---------------+---------------------------+---------------------------+
| ID                    | Name          |   Baremetal Project Name  |  Initial Image            |
+-----------------------+---------------+---------------------------+---------------------------+
| os-inception		| m1.small	|  os                         |  rhel7 or fedora27 qcow   |
| os-compute-2		| os.compute	|   os                      |  pxeboot.img              |
| os-compute-1		| os.compute	|   os                      |  pxeboot.img              |
| os-ceph-3		| os.ceph	|   os                      |  rhel7 or fedora27 qcow   |
| os-ceph-2		| os.ceph	|   os                      |  rhel7 or fedora27 qcow   |
| os-ceph-1		| os.ceph	|   os                      |  rhel7 or fedora27 qcow   |
| os-service-3		| os.service	|   os                      |  pxeboot.img              |
| os-service-2		| os.service	|   os                      |  pxeboot.img              |
| os-service-1		| os.service	|   os                      |  pxeboot.img              |
| os-pacemaker-3	| os.pacemaker	|   os                      |  pxeboot.img              |
| os-pacemaker-2	| os.pacemaker	|   os                      |  pxeboot.img              |
| os-pacemaker-1	| os.pacemaker	|   os                      |  pxeboot.img              |
| os-ospd-1		| os.ospd	|   os                      |  pxeboot.img              |
+-----------------------+---------------+---------------------------+---------------------------+

$ openstack flavor list
+--------------+-------+------+-----------+-------+-----------+
| Name         |   RAM | Disk | Ephemeral | VCPUs | Is Public |
+--------------+-------+------+-----------+-------+-----------+
| m1.small     |  2048 |   10 |         0 |     1 | True      |
| os.pacemaker |  8192 |   20 |         0 |     2 | False     |
| os.ospd      | 16384 |   40 |         0 |     4 | False     |
| os.compute   |  6144 |   10 |         0 |     4 | False     |
| os.ceph      |  4096 |   20 |         0 |     2 | False     |
| os.service   | 16384 |   10 |         0 |     2 | False     |
+--------------+-------+------+-----------+-------+-----------+

As configured in our tests, the total required space is for a single inception overcloud
Memory:		82  GiB
Disk:		200 GiB
Ephemeral: 	0   GiB
vCPU:		31

The os.* flavors will be created in the os project by the playbook.

License Prerequisites:
--------------
If installing Red Hat OpenStack Platform, Red Hat Ceph Storage, and RHEL, please be sure to comply with license requirements.

Inception instance OS and Software
--------------

OS:
- RHEL 7 (.3 or .4 tested)
- Fedora 27

Software:
- EPEL repository (if using RHEL)
- pip
- jq
- Ansible 2 (tested with 2.6.0)

Setup Instructions
==============
Requirements
- A system running RHEL7.
- Create an ssh key for the account you are using and add this into your gitlab account to grant access to the git repos you need to clone.
- The ```os-inception``` directory cloned to the home directory from the git branch ```keystonev3```

    ```
    git clone -b keystonev3 ssh://git@example.com:2222/UtilityCloud/os-inception.git
    ```
- The following packages installed: git, ansible, python-virtualenvwrapper, gcc,make, python-devel

    ```
    sudo yum install -y git ansible python-virtualenvwrapper gcc make python-devel
    ```

- A Python virtualenv activated with the following command

    ```  
    . /usr/bin/virtualenvwrapper.sh
    ```





This is probably a good time to update the clouddata repo with details for your new dev cloud.


## Create a new clouddata file

- Clone these files from the Utility cloud repository

    ```
    git clone ssh://git@example.com:2222/UtilityCloud/clouddata.git
    ```
- Create a copy of unclass yaml file for your inception cloud  i.e unclass_os1.yaml

    ```
    cd clouddata
    cp unclass_os10.yaml unclass_<inception cloud name>.yaml
    ```

- Update the copy of the file with the correct values for
  - ```domain_prefix```
  - ```hostnameformat```

- Push file back to git:
    ```
    git add -A
    git commit -m "Added new clouddata file"
    git push
    ```

## Create new branch for the icloud\_puppet repo in git named "unclass\_\<inception cloud name\>"


- In the clouddata repo ensure that a clouddata file exists that is named the same as your icloud_puppet_branch. E.g for the os1 inception cloud there is a branch in icloud_puppet called "unclass_os1" and a corresponding file ```unclass_os1.yaml``` in the clouddata repo that contains the cloud data for this dev cloud.


## Setup your openstack credentials on the jumphost  for the baremetal cloud you are using.

- Browse to the bare metal cloud horizon interface and login to the IPA domain with your IPA credentials.

- Select the grp-cloud_ops project.
- On the top tab select project and select "Project" -> "API Access"
- Click on "Download Openstack RC File (Identity API v3)" and save the file grp-cloud_ops-openrc.sh.
- Edit the file and add the path to the cacert to the baremetal cloud by adding a line
```
export OS_CACERT=/<path to your baremetal cloud cacert>
```

- Upload this file to your account on the jumphost.


## Run Ansible Playbooks

Run these playbooks in the ```os-inception/``` directory after the steps above have been completed.

- Run the playbook to setup the virtualenv Python environment. This playbook creates ```.virtualenvs/openstacksdk2``` if it doesn't exist already.

    ```
    ansible-playbook create-virtualenv.yml -vv
    ```

- Select this Python virtualenv to work on openstacksdk2. This is virtualenv contains all of the correct Python dependencies for the tasks in the Ansible playbooks that will be run.

    ```
    workon openstacksdk1
    ```

- Load your openstack credentials with.

```
. grp-cloud_ops-openrc.sh
```
  and enter your IPA password when prompted. It is a good idea at this point to confirm that your credentials are properly set up by running

```
openstack domain show
```


- Run the playbook to create the dev cloud tenant and bastion instance. Replace ```<project_name>``` with the name of your cloud/project for example ```os_project=os1```

    ```
    ansible-playbook create-tenant-and-bastion.yml -vv --extra-vars "os_project=<project_name">
    ```

- Note that in the case where you are using your own account to access the baremetal cloud you should edit ```group_vars/all/project``` to set and comment out the following lines

    - Set:  
        - ```use_running_user: true```
    - Comment out:
        - ```ansible_user```
        - ```ansible_ssh_pass```

- The ```create-tenant-and-bastion.yml``` playbook can then be run with the -k option to prompt for your user password.

    ```
    ansible-playbook create-tenant-and-bastion.yml -k -vv --extra-vars "os_project=<project_name>"
    ```

- After the ```create-tenant-and-bastion.yml``` has run, login to the newly created and configured bastion instance.

    ```
    ssh os1-bastion
    ```

- Change into the os-inception directory on the bastion instance and run the ```create-openstack-on-openstack.yml``` playbook

    ```
    ansible-playbook create-openstack-on-openstack.yml -vv --extra-vars "os_project=<project_name">
    ```



## Start Overcloud Deploy



- On the **Bastion Host** run the following playbook from within the ```os-inception``` directory.


    ```
    ansible-playbook deploy-overcloud.yml -vv --extra-vars "os_project=os1" --skip-tags overcloud-deploy
    ```
- SSH to the OSPD (OpenStack Platform Director)
    ```
    ssh stack@<project name>-ospd-1
    ```

- On the OSPD instance, source bashrc file with the undercloud credentials.

    ```
    . stackrc
    ```

- List "baremetal" nodes to confirm before running deploy script

    ```
    openstack baremetal node list
    ```

- Run deploy

    ```
    ./deploy-osp13.sh
    ```

- Deploy should complete in ~45mins

- On the **BASTION** instance run the following playbook to configure access to the overcloud from the bastion host:

    ```
    ansible-playbook overcloud-access-setup.yml  -vv --extra-vars "os_project=os1"
    ```

## Post Deploy


- You can then access the Horizon dashboard interface by editing the ssd_config on the bastion host as root

    ```
    vim /etc/ssh/sshd_config
    ```
- Set ```AllowTcpForwarding``` to yes and then restart sshd on the bastion. From your desktop run the following command before start the Firefox browser and setting the SOCKS proxy to localhost 8088. You will then be able to browse to ```https://10.0.0.100``` to access the Horizon dashboard interface of the development cloud.

    ```
    ssh -D 8080 <BASTION IP ADDRESS>
    ```

## Validate

Manual VM boot or Tempest

- Tempest can be run on the deployed cloud with

    ```
    ansible-playbook stack-tempest-deploy.yml -vv
    ```

- After running Tempest look at the log file in ```/home/stack/tempest```


**Note that the ```stack_post_deploy_all.yaml``` Ansible playbook should be run prior to the Tempest tests as it sets various parameters that Tempest needs to run. One extra parameter that needs to be applied across all of the services nodes that is not yet in ```stack_post_deploy_all.yaml``` is**

    crudini --set /etc/nova/nova.conf DEFAULT scheduler_default_filters RetryFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter,DifferentHostFilter
    systemctl restart openstack-nova-api.service openstack-nova-scheduler.service


## Troubleshooting

The playbooks use a rich collection of tags which allow limiting the tasks that are run. For example:

- To regenerated service instances only

    ```
    ansible-playbook delete-openstack-on-openstack.yml --tags service-instances
    ansible-playbook create-openstack-on-openstack.yml --tags service-instances
    ```

- To regenerate templates (eg after clouddata/icloud-puppet run)

    ```
    ansible-playbook create-openstack-on-openstack.yml --tags core,templates
    ```

- To recreate virtual ceph

    ```
    ansible-playbook delete-openstack-on-openstack.yml --tags ceph-instances
    ansible-playbook create-openstack-on-openstack.yml --tags ceph-instances
    ansible-playbook ceph-setup.yml
    ```

```core``` needs to be included for anything that uses dynamic inventory (anything that runs in OSPd for example)

```infrastructure``` is limited to IaaS bits - creates projects/networks/instances etc - basically no software or templating

Possibilities are endless. The user is encouraged to look through tags defined in the roles.


License
-------

    OS-Inception - automation for building openstack on openstack
    Copyright (C) 2017 Rarm Nagalingam at Red Hat, Inc

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.


Author Information
------------------

GitHub IDs and e-mails
- Rarm Nagalingam: snowjet
- Jacob Anders:  jacobanders
- Max D Bott:    mdbott  (mbott@redhat.com)
- John Apple II: 9strands ( jappleii -{at}- redhat -{dot}- com or john -{at}- theapplefamily -{dot}- info )
