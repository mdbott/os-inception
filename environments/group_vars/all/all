---
# Required configuration
# This MUST be reviewed and adjusted to match the environment the playbook will
# be running on

#### Keystone V3 only ###
#
## Pre-existing username and password
#ansible_user: ansible
#ansible_user_password: passw0rd
## OpenStack project to be used. Will be created unless create_project is set to "false"
#os_project: inception1
## OpenStack role in this project to be granted to the specified user. 
## This won't apply if create_project is set to "false"
#ansible_role: _member_
#
## If the two below values are set to false (and projects/flavors are 
## configured manually by the operator, the playbook doesn't need valid
## admin credentials. Values still need to be set to avoid empty value
## errors, but will not be used anywhere. This can be improved if needed
##
## setting this boolean will enable/disable:
## * creation of the project
## * additing the configured user to the project
## * setting quota
#create_project: true
## setting this boolean will enable/disable
## * creating custom flavors
## * adding access to these flavors to the project
#configure_flavors: true
#
## admin user name, password and role
#admin_user: eadmin
#admin_user_password: passw0rd
#admin_project: admin
#
## OpenStack cluster name
#os_cluster: baremetal
#os_cluster_admincreds: baremetal_admin
#
## keystone URL
#os_cluster_keystone_adminurl: https://baremetal:13000/v3
#
## OpenStack user domain name
#user_domain_name: mydomain
#
## OpenStack user domain ID
#user_domain_id: abcdeabceabceabcdeabcdeaabcdeabc
#
## OpenStack domain name
#project_domain_name: mydomain
#
## OpenStack project domain ID
#project_domain_id: edcbaedcbaedbcaedcbaedcbaedcbaed
#
## keystone API version
#identity_api_version: 3
#
#openstack_user_auth_env:
#  OS_NO_CACHE: True
#  COMPUTE_API_VERSION: 1.1
#  OS_USERNAME: "{{ ansible_user }}"
#  OS_PROJECT_NAME: "{{ os_project }}"
#  OS_CLOUDNAME: overcloud
#  OS_AUTH_URL: "{{ os_cluster_keystone_adminurl }}"
#  NOVA_VERSION: 1.1
#  OS_PASSWORD: "{{ ansible_user_password }}"
#  OS_CACERT: "{{ cacertfile }}"
#  OS_USER_DOMAIN_NAME: "{{ user_domain_name  }}"
#  OS_PROJECT_DOMAIN_NAME: "{{ project_domain_name  }}"
#  OS_IDENTITY_API_VERSION: "{{ identity_api_version  }}"
#
#openstack_admin_auth_env:
#  OS_NO_CACHE: True
#  COMPUTE_API_VERSION: 1.1
#  OS_USERNAME: "{{ admin_user }}"
#  OS_PROJECT_NAME: "{{ admin_project }}"
#  OS_CLOUDNAME: overcloud
#  OS_AUTH_URL: "{{ os_cluster_keystone_adminurl }}"
#  NOVA_VERSION: 1.1
#  OS_PASSWORD: "{{ admin_user_password }}"
#  OS_CACERT: "{{ cacertfile }}"
#  OS_USER_DOMAIN_NAME: "{{ user_domain_name  }}"
#  OS_PROJECT_DOMAIN_NAME: "{{ project_domain_name  }}"
#  OS_IDENTITY_API_VERSION: "{{ identity_api_version  }}"
#
## CA cert path and content (ansible will populate this on the ansible node)
#cacertfile: "/etc/pki/ca-trust/source/anchors/{{ os_cluster}}_ca.crt"
#cacert: |
#        -----BEGIN CERTIFICATE-----
#        -----END CERTIFICATE-----
#### End Keystone V3 only ###

### Keystone V2 only ###
## Pre-existing username and password
ansible_user: baremetal_project_user
ansible_user_password: passw0rd
# OpenStack project to be used. Will be created unless create_project is set to "false"
os_project: inception1
# OpenStack role in this project to be granted to the specified user. 
# This won't apply if create_project is set to "false"
ansible_role: _member_

# If the two below values are set to false (and projects/flavors are 
# configured manually by the operator, the playbook doesn't need valid
# admin credentials. Values still need to be set to avoid empty value
# errors, but will not be used anywhere. This can be improved if needed
#
# setting this boolean will enable/disable:
# * creation of the project
# * additing the configured user to the project
# * setting quota
create_project: true
# setting this boolean will enable/disable
# * creating custom flavors
# * adding access to these flavors to the project
configure_flavors: true

# admin user name, password and role
admin_user: admin
admin_user_password: passw0rd
admin_project: admin

# Ironic virtual power management user (this is used by the IPMI replacement
# script to authenticate to the baremetal nova. It must have the ability to
# start and to stop nova instances in the inception project
ironic_virtual_ipmi:
  user: baremetal_inceptionproject_user
  password: passw0rd

# NTP server
ntp_server: myntp.example.com

# OpenStack cluster name
os_cluster: baremetal
os_cluster_admincreds: baremetal_admin

# keystone URL
os_cluster_keystone_adminurl: https://baremetal.example.com:13357/v2.0
os_cluster_keystone_publicurl: https://baremetal.example.com:13000/v2.0

# keystone API version
identity_api_version: 2

## OpenStack user domain ID
user_domain_id: ""
### OpenStack project domain ID
project_domain_id: ""

# this is required by shell commands in roles/052-nova-flavors-access/ and can go away
#  as soon as os_flavor_access module is available
openstack_user_auth_env:
  OS_NO_CACHE: True
  COMPUTE_API_VERSION: 1.1
  OS_USERNAME: "{{ ansible_user }}"
  OS_PROJECT_NAME: "{{ os_project }}"
  OS_CLOUDNAME: overcloud
  OS_AUTH_URL: "{{ os_cluster_keystone_adminurl }}"
  NOVA_VERSION: 1.1
  OS_PASSWORD: "{{ ansible_user_password }}"
  OS_CACERT: "{{ cacertfile }}"

# this is required by shell commands in roles/052-nova-flavors-access/ and can go away
#  as soon as os_flavor_access module is available
openstack_admin_auth_env:
  OS_NO_CACHE: True
  COMPUTE_API_VERSION: 1.1
  OS_USERNAME: "{{ admin_user }}"
  OS_PROJECT_NAME: "{{ admin_project }}"
  OS_CLOUDNAME: overcloud
  OS_AUTH_URL: "{{ os_cluster_keystone_adminurl }}"
  NOVA_VERSION: 1.1
  OS_PASSWORD: "{{ admin_user_password }}"
  OS_CACERT: "{{ cacertfile }}"

# CA cert path and content (ansible will populate this on the ansible node)
cacertfile: "/etc/pki/ca-trust/source/anchors/baremetal-ca.crt"
cacert: |
        -----BEGIN CERTIFICATE-----
        -----END CERTIFICATE-----
osinception_cacertfile: "/etc/pki/ca-trust/source/anchors/{{ prefix }}-ca.crt"
osinception_cacert: |
        -----BEGIN CERTIFICATE-----
        -----END CERTIFICATE-----



## End Keystone V2 only ###

# pre-existing keypair which will be added on OSPd (does not apply to overcloud nodes 
# as those are yet to be built) the private key needs to be copied to the ansible node
key_name: ansible
key_file: ~/.ssh/id_rsa
key_content: |
  -----BEGIN RSA PRIVATE KEY-----
  -----END RSA PRIVATE KEY-----

# the network to connect OSPd to
internal_network: internal_management
external_network: external_net
ospd_float: 10.0.0.101

# pre-existing public image which will be used to build OSPd
#ospd_base_image: rhel-7.3-35
rhel_image: myimage.qcow2

# the custom PXE boot image used to boot the overcloud nodes from OSPd. This will be created with ansible
pxe_image: pxeboot

# activation key to be used
activation_key: satellite_registration_key


reposerver_url: http://yum.example.com

yum_repos:
  - rhel-7-server-rpms
  - rhel-7-server-rh-common-rpms
  - rhel-7-server-extras-rpms
  - rhel-7-server-satellite-tools-6.2-rpms
  - rhel-7-server-openstack-10-rpms
  - rhel-7-server-openstack-10-optools-rpms
  - rhel-7-server-openstack-10-tools-rpms
  - rhel-7-server-openstack-10-devtools-rpms 
  - rhel-7-server-rhscon-2-installer-rpms
  - rhel-7-server-rhceph-2-mon-rpms
  - rhel-7-server-rhceph-2-osd-rpms

yum_gpgcheck: yes
yum_gpgcheck_key: "file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release"

# satellite URL to be used
satellite_url: https://satellite6.example.com/pulp/repos

# Satellite organisation to be used
satellite_org: MyOrganization

# RPM with the Satellite keys
satellite_pkg: https://satellite6.example.com/pub/katello-ca-consumer-latest.noarch.rpm

# Timeout used when creating instances
instance_creation_timeout: 200

# this can be used to nominate nodes for running os-on-os instances. Set to "nova" if not required
instance_az: "nova"

git_repos:
  foreman: "ssh://git@gitlocation.server.example.com/Group/foreman.git"
  cloud: "ssh://git@gitlocation.server.example.com/Group/cloud.git"

use_existing_external_network: "false"
use_existing_ceph: "false"

# End required configuration

# The below is used to ensure the instances span multiple nodes for HA (pacemaker/loadbalanced services)
affinity_policy: "anti-affinity"
use_affinity_groups: "true"

# Internal configuration
# These values are considered "internal". Changes to this file are not required to get
# the playbook to work, but can be made by power-users

# The project user and role to be used by os-on-os. This is created in the admin playbook
# let's prefix everything with the project name, so that we can have multiple environments
prefix: "{{ os_project }}"

# Images which will be uploaded to the "host" cloud (running virtual cloud instances).
# Files need to be provided with the playbook
images:
  pxeboot:
    filename: pxeboot.img
    name: "{{ prefix }}_pxeboot"
    disk_format: raw
    container_format: bare
# this is commented out as we are not shipping a rhel7 qcow2 with the playbook. 
# if the file is shipped with the code in the appropriate task's "files" directory
# this can be uncommented
#  rhel:
#    filename: rhel-7.qcow2      
#    name: "{{ prefix }}_rhel-7"
#    disk_format: qcow2
#    container_format: bare

# Quota to be set for the OS-on-OS project by the playbook if create_project is true and
# valid admin creds are provided
quota:
  ram: 131072
  vcpus: 32
  instances: 16
  ports: 128

# a simple list of node classes, used for with_items loops in the tasks
node_types:
 - pacemaker
 - service
 - ceph
 - compute
 - ospd

# Flavors to be created by the plabook if create_flavors is true and valid admin creds are
# provided
flavors:
  pacemaker:
    name: "{{ prefix }}.pacemaker"
    vcpus: 2
    ram: 8192
    disk: 20
    ephemeral: 0
  service:
    name: "{{ prefix }}.service"
    vcpus: 2
    ram: 16384
    disk: 10
    ephemeral: 0
  compute:
    name: "{{ prefix }}.compute"
    vcpus: 4
    ram: 6144
    disk: 10
    ephemeral: 0
  ceph:
    name: "{{ prefix }}.ceph"
    vcpus: 2
    ram: 4096
    disk: 20
    ephemeral: 0
  ospd:
    name: "{{ prefix }}.ospd"
    vcpus: 4
    ram: 16384
    disk: 40
    ephemeral: 0

# BUG WARNING: using custom security groups with multiple interfaces, some of which are on port_security_disabled networks
#  can result in boot ERRORS. For this reason, we only use the default security group which seems to work somehow.
os_inception_secgroup: default


# node definition, used by the instance creation task
# node counts, connected networks, flavor, image and secgroups 
# can be customised here
nodes:
  pacemaker:
    flavor: "{{ prefix }}.pacemaker"
    image: "{{ prefix }}_pxeboot"
    secgroup: "{{ os_inception_secgroup }}"
    count: 3
    networks:
      - provisioning
      - storage
      - storagemgmt
      - internalapi
      - external
  service:
    flavor: "{{ prefix }}.service"
    image: "{{ prefix }}_pxeboot"
    secgroup: "{{ os_inception_secgroup }}"
    count: 3
    networks:
      - provisioning
      - storage
      - storagemgmt
      - internalapi
      - tenant
      - external
  ceph:
    flavor: "{{ prefix }}.ceph"
    image: "{{ rhel_image }}"
    secgroup: "{{ os_inception_secgroup }}"
    count: 3
    networks:
      - provisioning
      - storage
      - storagemgmt
  compute:
    flavor: "{{ prefix }}.compute" 
    image: "{{ prefix }}_pxeboot"
    secgroup: "{{ os_inception_secgroup }}"
    count: 2
    networks:
      - provisioning
      - storage
      - internalapi
      - tenant
  ospd:
    flavor: "{{ prefix }}.ospd"
    image: "{{ rhel_image }}"
    secgroup: "{{ os_inception_secgroup }}"
    count: 1
    networks:
      - provisioning
      - external

subnets_allocation_pool_start: 10
subnets_allocation_pool_end: 250

# network definitions used for network, subnet and port creation
# can be customised as required. These are the default OSPd values
networks:
  provisioning:
    name: "{{ prefix }}_provisioning"
    cidr: "192.0.2.0/24"
    netmask_length: 24
    cidr_prefix: "192.0.2"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: false
    no_gateway_ip: true
    pool_start: 10
    pool_end:  250
    use_existing: "false"
  internalapi:
    name: "{{ prefix }}_internalapi"
    cidr: "172.17.0.0/24"
    cidr_prefix: "172.17.0"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: true
    no_gateway_ip: true
    pool_start: 10
    pool_end:  250
    use_existing: "false"
  storage:
    name: "{{ prefix }}_storage"
    cidr: "172.18.0.0/24"
    cidr_prefix: "172.18.0"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: true
    no_gateway_ip: true
    pool_start: 10
    pool_end:  250
    use_existing: "false"
  storagemgmt:
    name: "{{ prefix }}_storagemgmt"
    cidr: "172.19.0.0/24"
    cidr_prefix: "172.19.0"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: true
    no_gateway_ip: true
    pool_start: 10
    pool_end:  250
    use_existing: "false"
  tenant:
    name: "{{ prefix }}_tenant"
    cidr: "172.16.0.0/24"
    cidr_prefix: "172.16.0"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: true
    no_gateway_ip: true
    pool_start: 10
    pool_end:  250
    use_existing: "false"
  external:
    name: "{{ prefix }}_external"
    cidr: "10.0.0.0/24"
    cidr_prefix: "10.0.0"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: true
    no_gateway_ip: true
    pool_start: 10
    pool_end: 250
    use_existing: "false"

# last octets used for configuring fixed_ips on ports belonging 
# to instances. Last octets are consistent in each network
last_octet:
  ospd:
    ospd_1: 253
  pacemaker:
    pacemaker_1: 11
    pacemaker_2: 12
    pacemaker_3: 13
  service:
    service_1: 14
    service_2: 15
    service_3: 16
  ceph:
    ceph_1: 17
    ceph_2: 18
    ceph_3: 19
  compute:
    compute_1: 20
    compute_2: 21


bastion_last_octet: 252

# this controls whether yum update will be run on the OSPd or not
ospd_install_updates: true

# packages required by the undercloud - plus some useful tools
undercloud_packages: 
  - python-tripleoclient 
  - openstack-utils
  - vim
  - strace
  - sysstat
  - screen
  - rhosp-director-images-ipa
  - rhosp-director-images

# undercloud definition - used for generating undercloud.conf
# this is based on OSPd defaults
undercloud:
  local_ip: "{{ networks.provisioning.cidr_prefix }}.1/{{ networks.provisioning.netmask_length }}"
  network_gateway: "{{ networks.provisioning.cidr_prefix }}.1"
  undercloud_public_vip: "{{ networks.provisioning.cidr_prefix }}.1"
  undercloud_admin_vip: "{{ networks.provisioning.cidr_prefix }}.1"
  local_interface: eth1
  local_mtu: 1500
  network_cidr: "{{ networks.provisioning.cidr }}"
  masquerade_network: "{{ networks.provisioning.cidr }}"
  dhcp_start: "{{ networks.provisioning.cidr_prefix }}.5"
  dhcp_end: "{{ networks.provisioning.cidr_prefix }}.24"
  undercloud_admin_password: "undercloud_password"

# undercloud environment variables used for running openstack commands
# on the undercloud using shell module
undercloud_auth_env_v2:
  NOVA_VERSION: 1.1
  OS_PASSWORD: "{{ undercloud.undercloud_admin_password }}"
  OS_AUTH_URL: "http://{{ undercloud.undercloud_public_vip }}:5000/v2.0"
  OS_USERNAME: admin
  OS_TENANT_NAME: admin
  COMPUTE_API_VERSION: 1.1
  OS_BAREMETAL_API_VERSION: 1.15
  OS_NO_CACHE: True
  OS_CLOUDNAME: undercloud
  OS_IMAGE_API_VERSION: 1

# overcloud image files which will be unpacked and uploaded to undercloud glance 
overcloud_image_files:
  - /usr/share/rhosp-director-images/overcloud-full-latest-10.0.tar 
  - /usr/share/rhosp-director-images/ironic-python-agent-latest-10.0.tar

# yum packages which will be installed on the ansible node
yum_prerequisites:
  - ansible-openstack-modules
  - python2-pip
  - patch
  - python-virtualenv
  - gcc
  - make
  - rubygems
  - qemu-img
  - vim
  - python-openstackclient

# pip packages which will be installed on the ansible node
pip_prerequisites:
  - shade

# misc internal settings. These shouldn't need to be changed
instance_wait: false
port_wait: false
yum_timeout: 600
yum_poll: 5
stack_user: stack

# End internal configuration
