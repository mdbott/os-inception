---
# Required configuration
# This MUST be reviewed and adjusted to match the environment the playbook will
# be running on


## OpenStack role in this project to be granted to the specified user.
## This won't apply if create_project is set to "false"
ansible_role: _member_
#
## If the two below values are set to false (and projects/flavors are
## configured manually by the operator, the playbook doesn't need valid
## admin credentials. Values still need to be set to avoid empty value
## errors, but will not be used anywhere. This can be improved if needed
##
## setting this boolean will enable/disable:
## * creation of the project
## * additing the configured user to the project
## * setting quota
create_project: true
## setting this boolean will enable/disable
## * creating custom flavors
## * adding access to these flavors to the project
configure_flavors: true

## setting this boolean will enable/disable
## * Updating the nested host aggregate with the proect id of the new devcloud project
## * adding the project filter metadata to the compute flavors to ensure they \
##    are scheduled onto an hypervisor with nest virt.
isolate_nested_computes: false

# Name of the nova host aggregate containing the compute nodes with nested virt enabled
inception_host_aggregate: nested
#
# Ironic virtual power management user
ironic_virtual_ipmi:
  user: "{{ os_username }}"
  password: "{{ os_password }}"

# NTP server
ntp_server: myntp.example.com
#
domain_name: example.com
## OpenStack cluster name
os_cluster: lab
os_cluster_admincreds: default
#
## keystone URL
use_running_user: false
undercloud_admin_password: 1800redhat
os_username: "{{ lookup('env', 'OS_USERNAME') }}"
os_password: "{{ lookup('env', 'OS_PASSWORD') }}"
os_project_name: "{{ lookup('env', 'OS_PROJECT_NAME') }}"
os_user_domain_name: "{{ lookup('env', 'OS_USER_DOMAIN_NAME') }}"
os_project_domain_name: "{{ os_user_domain_name }}"
os_project_id: "{{ lookup('env', 'OS_PROJECT_ID') }}"

os_cluster_keystone_publicurl: "{{ lookup('env', 'OS_AUTH_URL') }}"
os_cluster_keystone_adminurl: "{{ os_cluster_keystone_publicurl }}"
# keystone API version
identity_api_version: "{{ lookup('env', 'OS_IDENTITY_API_VERSION')|int }}"
# Endpoint to use for cloud interactions. Use 'admin' for keystone v2 and public for keystone v3
endpoint_type: "{{ lookup('env', 'OS_INTERFACE') }}"


ansible_user: "{{ os_username }}"
ansible_ssh_pass: "{{ os_password }}"
# admin user name, password and role
admin_user: "{{ os_username }}"
admin_user_password: "{{ os_password }}"
admin_project: "{{ os_project_name }}"
admin_domain_name: "{{ os_project_domain_name }}"
admin_user_domain_name: "{{ os_user_domain_name }}"
admin_project_domain_name: "{{ os_project_domain_name }}"
project_domain_name: "{{ os_project_domain_name }}"
bastion_user: cloud-user
bastion_user_group: "{{ bastion_user }}"
ospd_user: cloud-user
ospd_user_group: cloud-user

icloud_puppet_branch: "unclass_{{ os_project }}"
#
## OpenStack user domain name
user_domain_name: "{{ os_user_domain_name }}"
#
## Compute API version
compute_api_version: 2
#
## CA cert path and content (ansible will populate this on the ansible node)

openstack_user_auth_env_v2:
  OS_NO_CACHE: True
  COMPUTE_API_VERSION: 2.1
  OS_USERNAME: "{{ os_username }}"
  OS_PROJECT_NAME: "{{ os_project }}"
  OS_CLOUDNAME: overcloud
  OS_AUTH_URL: "{{ os_cluster_keystone_adminurl }}"
  NOVA_VERSION: 1.1
  OS_PASSWORD: "{{ os_password }}"
  OS_CACERT: "{{ cacertfile }}"

openstack_user_auth_env_v3:
  OS_USER_DOMAIN_NAME: "{{ os_user_domain_name }}"
  OS_PROJECT_DOMAIN_NAME: "{{ os_project_domain_name }}"
  OS_IDENTITY_API_VERSION: "{{ identity_api_version  }}"
  OS_INTERFACE: "{{ endpoint_type }}"

# this is required by shell commands in roles/070-nova-flavors-access/ and can go away
#  as soon as os_flavor_access module is available
openstack_admin_auth_env_v2:
  OS_NO_CACHE: True
  COMPUTE_API_VERSION: 2.1
  OS_USERNAME: "{{ os_username }}"
  OS_PROJECT_NAME: "{{ os_project_name }}"
  OS_CLOUDNAME: overcloud
  OS_AUTH_URL: "{{ os_cluster_keystone_adminurl }}"
  NOVA_VERSION: 1.1
  OS_PASSWORD: "{{ admin_user_password }}"
  OS_CACERT: "{{ cacertfile }}"

openstack_admin_auth_env_v3:
  OS_PROJECT_ID: "{{ os_project_id }}"
  OS_USER_DOMAIN_NAME: "{{ os_user_domain_name  }}"
  OS_PROJECT_DOMAIN_NAME: "{{ os_project_domain_name  }}"
  OS_IDENTITY_API_VERSION: "{{ identity_api_version  }}"
  OS_INTERFACE: public

cacertfile: "/etc/pki/ca-trust/source/anchors/{{ os_cluster }}-ca.crt"
cacert: |
        -----BEGIN CERTIFICATE-----
        -----END CERTIFICATE-----


# pre-existing keypair which will be added on OSPd (does not apply to overcloud nodes
# as those are yet to be built) the private key needs to be copied to the ansible node
key_name: "{{ os_project }}-cloud-access"
key_file: "/home/{{ bastion_user }}/.ssh/id_rsa.osdev{{ os_project }}"
key_content: |
  -----BEGIN RSA PRIVATE KEY-----
  -----END RSA PRIVATE KEY-----


external_network: public
internet_access: true
cloud_access: public
cloud_access_network: internal_management
cloud_access_subnet: "192.168.27."
cloud_access_last_octet: "{{ os_project[-1]|int * 2 +30}}"
ospd_cloud_access_last_octet: "{{ os_project[-1]|int * 2 +31}}"
cloud_access_ip_address: "{{ cloud_access_subnet}}{{cloud_access_last_octet}}"
ospd_cloud_access_ip_address: "{{ cloud_access_subnet}}{{ospd_cloud_access_last_octet}}"


# pre-existing public image which will be used to build OSPd
#ospd_base_image: rhel-7.3-35
rhel_image: "{{ os_project }}-rhel-server-7.5-x86_64-kvm.qcow2"

# the custom PXE boot image used to boot the overcloud nodes from OSPd. This will be created with ansible
pxe_image: pxeboot

# activation key to be used
activation_key: satellite_registration_key


reposerver_url: http://yum.example.com

# No default is provided. This should be supplied at the command line with_items
# extra_vars
osp_version: 13

yum_repos:
  - rhel-7-server-extras-rpms
  - rhel-7-server-openstack-{{ osp_version }}-devtools-rpms
  - rhel-7-server-openstack-{{ osp_version }}-optools-rpms
  - rhel-7-server-openstack-{{ osp_version }}-rpms
  - rhel-7-server-rh-common-rpms
  - rhel-7-server-rpms
  - rhel-7-server-satellite-tools-6.2-rpms
  - rhel-7-server-openstack-{{ osp_version }}-tools-rpms
  - rhel-7-server-rhceph-3-tools-rpms
  - rhel-7-server-rhceph-3-mon-rpms
  - rhel-ha-for-rhel-7-server-rpms

bastion_yum_repos:
  - epel
  - rhel-7-server-extras-rpms
  - rhel-7-server-openstack-10-devtools-rpms
  - rhel-7-server-openstack-10-optools-rpms
  - rhel-7-server-openstack-10-rpms
  - rhel-7-server-rh-common-rpms
  - rhel-7-server-rpms
  - rhel-7-server-satellite-tools-6.2-rpms
  - rhel-7-server-openstack-10-tools-rpms
  - rhel-7-server-rhceph-3-tools-rpms
  - rhel-7-server-rhceph-3-mon-rpms


yum_gpgcheck: no
yum_gpgcheck_key: "file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release"

# satellite URL to be used
satellite_url: 'http://sat6.example.com'

# Satellite organisation to be used
satellite_org: MyOrganization

# RPM with the Satellite keys
satellite_pkg: https://satellite6.example.com/pub/katello-ca-consumer-latest.noarch.rpm

# Timeout used when creating instances
instance_creation_timeout: 200

# this can be used to nominate nodes for running os-on-os instances. Set to "nova" if not required
instance_az: "nova"



# this indicates what git repos need to be cloned in order to install the components required for MaxHammer
# make sure that your ansible host can clone the repo (add the appropriate keys!)

git_repo_server: gitlab.consulting.redhat.com

git_repos:
  foreman: "ssh://git@gitlocation.server.example.com/Group/foreman.git"
  icloud: "ssh://git@gitlocation.server.example:2222/Group/icloud-puppet.git"
  ceph_ansible:
    url: "ssh://git@gitlocation.server.example:2222/Group/ceph-ansible.git"
    branch: "v3.1.10"
  os_inception:
    url: "ssh://git@gitlocation.server.example:2222/Group/os-inception.git"
    branch: "keystonev3"
  vendordata:
    url: "ssh://git@gitlocation.server.example:2222/Group/vendordata.git"
    branch: "v0.2.0"
  openstacksdk:
    url: "git+ssh://git@gitlocation.server.example:2222/Group/openstacksdk.git"
    branch: "master"
  ansible:
    url: "git+ssh://git@gitlocation.server.example:2222/Group/ansible.git"
    branch: "devel"

r10k_environment: icloud_{{ icloud_puppet_branch }}

use_existing_external_network: "false"
use_existing_ceph: "false"

# End required configuration

# The below is used to ensure the instances span multiple nodes for HA (pacemaker/loadbalanced services)
affinity_policy: "anti-affinity"
use_affinity_groups: "true"

# Internal configuration
# These values are considered "internal". Changes to this file are not required to get
# the playbook to work, but can be made by power-users

# The project user and role to be used by os-on-os. This is created in the admin playbook
# let's prefix everything with the project name, so that we can have multiple environments
prefix: "{{ os_project }}"

# Images which will be uploaded to the "host" cloud (running virtual cloud instances).
# Files need to be provided with the playbook
images:
  pxeboot:
    filename: pxeboot.img
    name: "{{ prefix }}_pxeboot"
    disk_format: raw
    container_format: bare
    properties:
      hw_rng_model: virtio
      os_shutdown_timeout: 30
remote_images:
  rhel-server-7.4-x86_64:
    url: http://repos.example.com/images/rhel-server-7.4-update-4-x86_64-kvm.qcow2
    filename: rhel-server-7.4-x86_64-kvm.qcow2
    name: "{{ os_project }}-rhel-server-7.4-x86_64-kvm"
    disk_format: qcow2
    container_format: bare
    properties:
      hw_rng_model: virtio
  rhel-server-7.5-x86_64:
    url: http://repos.example.com/images/rhel-server-7.5-update-3-x86_64-kvm.qcow2
    filename: rhel-server-7.5-x86_64-kvm.qcow2
    name: "{{ os_project }}-rhel-server-7.5-x86_64-kvm"
    disk_format: qcow2
    container_format: bare
    properties:
      hw_rng_model: virtio

# Quota to be set for the OS-on-OS project by the playbook if create_project is true and
# valid admin creds are provided
quota:
  ram: 200000
  vcpus: 50
  instances: 20
  ports: 250

overcloud_rootpw: passw0rd

# a simple list of node classes, used for with_items loops in the tasks
node_types:
 - bastion
 - pacemaker
 - service
 - ceph
 - compute
 - ospd

preprovisioned_nodes: false

# Flavors to be created by the plabook if create_flavors is true and valid admin creds are
# provided
flavors:
  bastion:
    name: "{{ prefix }}.bastion"
    vcpus: 2
    ram: 8192
    disk: 20
    ephemeral: 0
    extra_specs: "hw_rng:allowed=True"
  pacemaker:
    name: "{{ prefix }}.pacemaker"
    vcpus: 2
    ram: 8192
    disk: 40
    ephemeral: 0
    extra_specs: "hw_rng:allowed=True"
  service:
    name: "{{ prefix }}.service"
    vcpus: 2
    ram: 16384
    disk: 40
    ephemeral: 0
    extra_specs: "hw_rng:allowed=True"
  compute:
    name: "{{ prefix }}.compute"
    vcpus: 4
    ram: 6144
    disk: 40
    ephemeral: 0
    extra_specs: "hw_rng:allowed=True"
  ceph:
    name: "{{ prefix }}.ceph"
    vcpus: 2
    ram: 4096
    disk: 20
    ephemeral: 0
    extra_specs: "hw_rng:allowed=True"
  ospd:
    name: "{{ prefix }}.ospd"
    vcpus: 8
    ram: 16384
    disk: 80
    ephemeral: 0
    extra_specs: "hw_rng:allowed=True"

flavors_nested:
  bastion:
    name: "{{ prefix }}.bastion"
    vcpus: 2
    ram: 8192
    disk: 20
    ephemeral: 0
    extra_specs: "hw_rng:allowed=True"
  pacemaker:
    name: "{{ prefix }}.pacemaker"
    vcpus: 2
    ram: 8192
    disk: 20
    ephemeral: 0
    extra_specs: "hw_rng:allowed=True"
  service:
    name: "{{ prefix }}.service"
    vcpus: 2
    ram: 16384
    disk: 10
    ephemeral: 0
    extra_specs: "hw_rng:allowed=True"
  compute:
    name: "{{ prefix }}.compute"
    vcpus: 4
    ram: 6144
    disk: 10
    ephemeral: 0
    extra_specs:
      "hw_rng:allowed": "True"
      aggregate_instance_extra_specs:
        filter_tenant_id: "{{ os_project_id }}"
  ceph:
    name: "{{ prefix }}.ceph"
    vcpus: 2
    ram: 4096
    disk: 20
    ephemeral: 0
    extra_specs: "hw_rng:allowed=True"
  ospd:
    name: "{{ prefix }}.ospd"
    vcpus: 4
    ram: 16384
    disk: 80
    ephemeral: 0
    extra_specs: "hw_rng:allowed=True"

# BUG WARNING: using custom security groups with multiple interfaces, some of which are on port_security_disabled networks
#  can result in boot ERRORS. For this reason, we only use the default security group which seems to work somehow.
os_inception_secgroup: default


# node definition, used by the instance creation task
# node counts, connected networks, flavor, image and secgroups
# can be customised here
nodes:
  bastion:
    flavor: "{{ prefix }}.bastion"
    image: "{{ remote_images['rhel-server-7.5-x86_64'].name }}"
    secgroup: "{{ os_inception_secgroup }}"
    count: 1
    auto_ip: no
    wait: true
    networks:
      - provisioning
      - storage
      - internalapi
      - external
  pacemaker:
    flavor: "{{ prefix }}.pacemaker"
    image: "{{ prefix }}_pxeboot"
    secgroup: "{{ os_inception_secgroup }}"
    count: 3
    auto_ip: no
    wait: false
    networks:
      - provisioning
      - storage
      - storagemgmt
      - internalapi
      - external
  service:
    flavor: "{{ prefix }}.service"
    image: "{{ prefix }}_pxeboot"
    secgroup: "{{ os_inception_secgroup }}"
    count: 3
    auto_ip: no
    wait: false
    networks:
      - provisioning
      - storage
      - storagemgmt
      - internalapi
      - tenant
      - external
  ceph:
    flavor: "{{ prefix }}.ceph"
    image: "{{ remote_images['rhel-server-7.5-x86_64'].name }}"
    secgroup: "{{ os_inception_secgroup }}"
    count: 3
    auto_ip: no
    wait: false
    networks:
      - external
      - storage
      - storagemgmt
    volumes: 2
    volsize: 10
  compute:
    flavor: "{{ prefix }}.compute"
    image: "{{ prefix }}_pxeboot"
    secgroup: "{{ os_inception_secgroup }}"
    count: 2
    auto_ip: no
    wait: false
    networks:
      - provisioning
      - storage
      - internalapi
      - tenant
      - external
  ospd:
    flavor: "{{ prefix }}.ospd"
    image: "{{ remote_images['rhel-server-7.4-x86_64'].name }}"
    secgroup: "{{ os_inception_secgroup }}"
    count: 1
    auto_ip: no
    wait: true
    networks:
      - provisioning
      - external

subnets_allocation_pool_start: 30
subnets_allocation_pool_end: 250

# network definitions used for network, subnet and port creation
# can be customised as required. These are the default OSPd values
networks:
  provisioning:
    name: "{{ prefix }}_provisioning"
    cidr: "192.0.2.0/24"
    netmask_length: 24
    cidr_prefix: "192.0.2"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: false
    no_gateway_ip: true
    gateway_ip:
    dns_nameservers: []
    pool_start: 10
    pool_end:  250
    inspection_start: 130
    inspection_end: 150
    use_existing: "false"
  internalapi:
    name: "{{ prefix }}_internalapi"
    cidr: "172.21.0.0/24"
    cidr_prefix: "172.21.0"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: true
    no_gateway_ip: true
    gateway_ip:
    dns_nameservers: []
    pool_start: 10
    pool_end:  250
    use_existing: "false"
  storage:
    name: "{{ prefix }}_storage"
    cidr: "172.22.0.0/24"
    cidr_prefix: "172.22.0"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: true
    no_gateway_ip: true
    gateway_ip:
    dns_nameservers: []
    pool_start: 10
    pool_end:  250
    use_existing: "false"
  storagemgmt:
    name: "{{ prefix }}_storagemgmt"
    cidr: "172.19.0.0/24"
    cidr_prefix: "172.19.0"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: true
    no_gateway_ip: true
    gateway_ip:
    dns_nameservers: []
    pool_start: 10
    pool_end:  250
    use_existing: "false"
  tenant:
    name: "{{ prefix }}_tenant"
    cidr: "172.16.0.0/24"
    cidr_prefix: "172.16.0"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: true
    no_gateway_ip: true
    gateway_ip:
    dns_nameservers: []
    pool_start: 10
    pool_end:  250
    use_existing: "false"
  external:
    name: "{{ prefix }}_external"
    cidr: "10.0.0.0/24"
    cidr_prefix: "10.0.0"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: true
    no_gateway_ip: false
    gateway_ip: 10.0.0.1
    dns_nameservers: ["8.8.8.8"]
    pool_start: 10
    pool_end: 250
    use_existing: "false"

# last octets used for configuring fixed_ips on ports belonging
# to instances. Last octets are consistent in each network
last_octet:
  bastion:
    bastion_1: 252
  ospd:
    ospd_1: 253
  pacemaker:
    pacemaker_1: 31
    pacemaker_2: 32
    pacemaker_3: 33
  service:
    service_1: 14
    service_2: 15
    service_3: 16
  ceph:
    ceph_1: 17
    ceph_2: 18
    ceph_3: 19
  compute:
    compute_1: 20
    compute_2: 21




# this controls whether yum update will be run on the OSPd or not
ospd_install_updates: true

# packages required by the undercloud - plus some useful tools
undercloud_packages:
  - python-tripleoclient
  - openstack-utils
  - vim
  - strace
  - sysstat
  - screen
  - rhosp-director-images-ipa
  - rhosp-director-images

undercloud_services:
  - httpd
  - neutron-dhcp-agent
  - neutron-openvswitch-agent
  - neutron-server
  - openstack-aodh-evaluator
  - openstack-aodh-listener
  - openstack-aodh-notifier
  - openstack-ceilometer-central
  - openstack-ceilometer-collector
  - openstack-ceilometer-notification
  - openstack-glance-api
  - openstack-glance-registry
  - openstack-heat-api-cfn
  - openstack-heat-api
  - openstack-heat-engine
  - openstack-ironic-api
  - openstack-ironic-conductor
  - openstack-ironic-inspector-dnsmasq
  - openstack-ironic-inspector
  - openstack-mistral-api
  - openstack-mistral-engine
  - openstack-mistral-executor
  - openstack-nova-api
  - openstack-nova-cert
  - openstack-nova-compute
  - openstack-nova-conductor
  - openstack-nova-scheduler
  - openstack-swift-account-reaper
  - openstack-swift-account
  - openstack-swift-container-updater
  - openstack-swift-container
  - openstack-swift-object-updater
  - openstack-swift-object
  - openstack-swift-proxy
  - openstack-zaqar
  - openstack-zaqar@1

# undercloud definition - used for generating undercloud.conf
# this is based on OSPd defaults
undercloud:
  local_ip: "{{ networks.provisioning.cidr_prefix }}.1/{{ networks.provisioning.netmask_length }}"
  network_gateway: "{{ networks.provisioning.cidr_prefix }}.1"
  undercloud_public_vip: "{{ networks.provisioning.cidr_prefix }}.2"
  undercloud_admin_vip: "{{ networks.provisioning.cidr_prefix }}.3"
  local_interface: eth1
  local_mtu: 1500
  network_cidr: "{{ networks.provisioning.cidr }}"
  masquerade_network: "{{ networks.provisioning.cidr }}"
  dhcp_start: "{{ networks.provisioning.cidr_prefix }}.105"
  dhcp_end: "{{ networks.provisioning.cidr_prefix }}.124"
  inspection_start: "{{ networks.provisioning.cidr_prefix }}.{{ networks.provisioning.inspection_start }}"
  inspection_end: "{{ networks.provisioning.cidr_prefix }}.{{ networks.provisioning.inspection_end }}"
  undercloud_admin_password: "{{ undercloud_admin_password }}"
  # Options are 'none' (no encryption), 'localca', 'ipa'
  undercloud_encryption: none
  use_novajoin: false
  container_registry: registry.access.redhat.com/rhosp13

# undercloud environment variables used for running openstack commands
# on the undercloud using shell module
undercloud_auth_env_v2:
  NOVA_VERSION: 1.1
  OS_PASSWORD: "{{ undercloud_admin_password }}"
  OS_AUTH_URL: "http://{{ undercloud.network_gateway }}:5000/v2.0"
  OS_USERNAME: admin
  OS_TENANT_NAME: admin
  COMPUTE_API_VERSION: 1.1
  OS_BAREMETAL_API_VERSION: 1.15
  OS_NO_CACHE: True
  OS_CLOUDNAME: undercloud
  OS_IMAGE_API_VERSION: 1


undercloud_auth_env_v3:
  NOVA_VERSION: 1.1
  OS_PASSWORD: "{{ undercloud_admin_password }}"
  OS_AUTH_URL: "http://{{ undercloud.network_gateway }}:5000/"
  OS_USERNAME: admin
  OS_AUTH_TYPE: password
  OS_PROJECT_NAME: admin
  OS_USER_DOMAIN_NAME: Default
  OS_PROJECT_DOMAIN_NAME: Default
  COMPUTE_API_VERSION: 1.1
  OS_BAREMETAL_API_VERSION: 1.34
  OS_NO_CACHE: True
  OS_CLOUDNAME: undercloud
  OS_IDENTITY_API_VERSION: 3


# overcloud image files which will be unpacked and uploaded to undercloud glance
overcloud_image_files:
  - /usr/share/rhosp-director-images/overcloud-full-latest-{{ osp_version }}.0.tar
  - /usr/share/rhosp-director-images/ironic-python-agent-latest-{{ osp_version }}.0.tar

# yum packages which will be installed on the ansible node
yum_prerequisites:
  - git
  - patch
  - gcc
  - make
  - rubygems
  - rubygem-bundler
  - qemu-img
  - jq
  - vim
  - python-devel
  - sshpass
  - python-dns
  - unzip


getpip_url: https://bootstrap.pypa.io/get-pip.py


virtualenv_name: openstacksdk1

pip_module_url: http://repos.gpslab.cbr.redhat.com/modules/

pip_version: "18.0"

pip_trusted_host: repos.gpslab.cbr.redhat.com

# pip packages which will be installed on the ansible node
old_pip_prerequisites:
  - yq
  - python-openstackclient
  - shade==1.26
  - ansible==2.5.3

pip_prerequisites:
  - python-openstackclient
  - python-neutronclient
  - openstacksdk==0.17
  - ansible==2.7.8
  - stevedore==1.27
  - requests==2.18.0
  - keystoneauth1==3.9.0
  - ipaddress==1.0.17
  - notario
  - decorator==4.4.0


pip_conflicting_rpms:
  - pyOpenSSL
  - PyYAML
  - python-requests
  - python-ipaddress
  - python-inotify


# Install gems from a network url or use the local versions.
gem_install: true

# gem source url of the form http://<gem repo server>/artifactory/api/gems/gems/
gemsource_url: https://rubygems.org/

# misc internal settings. These shouldn't need to be changed

port_wait: false
yum_timeout: 1800
yum_poll: 5
stack_user: stack

# End internal configuration

keystone_service_adminuri_insecure: false
keystone_service_internaluri_insecure: false
openrc_os_auth_url: "{{ os_cluster_keystone_adminurl }}"
openrc_os_tenant_name: "{{ admin_project }}"
openrc_os_password: "{{ admin_user_password }}"
openrc_os_domain_name: "{{ admin_domain_name  }}"
openrc_clouds_yml_interface: public
