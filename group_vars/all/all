---
# Required configuration
# This MUST be reviewed and adjusted to match the environment the playbook will
# be running on


## OpenStack role in this project to be granted to the specified user.
## This won't apply if create_project is set to "false"
ansible_role: _member_
#
## If the two below values are set to false (and projects/flavors are
## configured manually by the operator, the playbook doesn't need valid
## admin credentials. Values still need to be set to avoid empty value
## errors, but will not be used anywhere. This can be improved if needed
##
## setting this boolean will enable/disable:
## * creation of the project
## * additing the configured user to the project
## * setting quota
create_project: true
## setting this boolean will enable/disable
## * creating custom flavors
## * adding access to these flavors to the project
configure_flavors: true

## setting this boolean will enable/disable
## * Updating the nested host aggregate with the proect id of the new devcloud project
## * adding the project filter metadata to the compute flavors to ensure they \
##    are scheduled onto an hypervisor with nest virt.
isolate_nested_computes: false

# Name of the nova host aggregate containing the compute nodes with nested virt enabled
inception_host_aggregate: nested
#
# Ironic virtual power management user
ironic_virtual_ipmi:
  user: ansible
  password: passw0rd

# NTP server
ntp_server: myntp.example.com
#
domain_name: cbr.lab
## OpenStack cluster name
os_cluster: iron
os_cluster_admincreds: default
#
## keystone URL

os_cluster_keystone_adminurl: https://baremetal.example.com:13357/v2.0
os_cluster_keystone_publicurl: https://baremetal.example.com:13357/v2.0

# keystone API version
identity_api_version: 2
# Endpoint to use for cloud interactions. Use 'admin' for keystone v2 and public for keystone v3
endpoint_type: admin

#
## OpenStack user domain name
user_domain_name: org
#
## Compute API version
compute_api_version: 2
#
## CA cert path and content (ansible will populate this on the ansible node)

openstack_user_auth_env_v2:
  OS_NO_CACHE: True
  COMPUTE_API_VERSION: 2.1
  OS_USERNAME: "{{ ansible_user }}"
  OS_PROJECT_NAME: "{{ os_project }}"
  OS_CLOUDNAME: overcloud
  OS_AUTH_URL: "{{ os_cluster_keystone_adminurl }}"
  NOVA_VERSION: 1.1
  OS_PASSWORD: "{{ ansible_ssh_pass }}"
  OS_CACERT: "{{ cacertfile }}"

openstack_user_auth_env_v3:
  OS_USER_DOMAIN_NAME: "{{ user_domain_name }}"
  OS_PROJECT_DOMAIN_NAME: "{{ project_domain_name }}"
  OS_IDENTITY_API_VERSION: "{{ identity_api_version  }}"
  OS_INTERFACE: public

# this is required by shell commands in roles/070-nova-flavors-access/ and can go away
#  as soon as os_flavor_access module is available
openstack_admin_auth_env_v2:
  OS_NO_CACHE: True
  COMPUTE_API_VERSION: 2.1
  OS_USERNAME: "{{ admin_user }}"
  OS_PROJECT_NAME: "{{ admin_project }}"
  OS_CLOUDNAME: overcloud
  OS_AUTH_URL: "{{ os_cluster_keystone_adminurl }}"
  NOVA_VERSION: 1.1
  OS_PASSWORD: "{{ admin_user_password }}"
  OS_CACERT: "{{ cacertfile }}"

openstack_admin_auth_env_v3:
  OS_PROJECT_ID: "{{ admin_project_id }}"
  OS_USER_DOMAIN_NAME: "{{ admin_domain_name  }}"
  OS_PROJECT_DOMAIN_NAME: "{{ admin_domain_name  }}"
  OS_IDENTITY_API_VERSION: "{{ identity_api_version  }}"
  OS_INTERFACE: public

cacertfile: "/etc/pki/ca-trust/source/anchors/{{ os_cluster }}-ca.crt"
cacert: |
        -----BEGIN CERTIFICATE-----
        -----END CERTIFICATE-----


# pre-existing keypair which will be added on OSPd (does not apply to overcloud nodes
# as those are yet to be built) the private key needs to be copied to the ansible node
key_name: "{{ os_project }}-cloud-access"
key_file: "/home/{{ bastion_user }}/.ssh/id_rsa.osdev{{ os_project }}"
key_content: |
  -----BEGIN RSA PRIVATE KEY-----
  -----END RSA PRIVATE KEY-----


external_network: external_net

cloud_access: private
cloud_access_network: internal_management
cloud_access_subnet: "192.168.27."
cloud_access_last_octet: "{{ os_project[-1]|int * 2 +30}}"
ospd_cloud_access_last_octet: "{{ os_project[-1]|int * 2 +31}}"
cloud_access_ip_address: "{{ cloud_access_subnet}}{{cloud_access_last_octet}}"
ospd_cloud_access_ip_address: "{{ cloud_access_subnet}}{{ospd_cloud_access_last_octet}}"


# pre-existing public image which will be used to build OSPd
#ospd_base_image: rhel-7.3-35
rhel_image: "{{ os_project }}-rhel-server-7.5-x86_64-kvm.qcow2"

# the custom PXE boot image used to boot the overcloud nodes from OSPd. This will be created with ansible
pxe_image: pxeboot

# activation key to be used
activation_key: satellite_registration_key


reposerver_url: http://yum.example.com

osp_version: 10

yum_repos:
  - rhel-7-server-extras-rpms
  - rhel-7-server-openstack-{{ osp_version }}-devtools-rpms
  - rhel-7-server-openstack-{{ osp_version }}-optools-rpms
  - rhel-7-server-openstack-{{ osp_version }}-rpms
  - rhel-7-server-rh-common-rpms
  - rhel-7-server-rpms
  - rhel-7-server-satellite-tools-6.2-rpms
  - rhel-7-server-openstack-{{ osp_version }}-tools-rpms
  - rhel-7-server-rhscon-2-installer-rpms
  - rhel-7-server-rhceph-2-mon-rpms
  - rhel-7-server-rhceph-2-osd-rpms
  - rhel-ha-for-rhel-7-server-rpms

bastion_yum_repos:
  - epel
  - rhel-7-server-extras-rpms
  - rhel-7-server-openstack-10-devtools-rpms
  - rhel-7-server-openstack-10-optools-rpms
  - rhel-7-server-openstack-10-rpms
  - rhel-7-server-rh-common-rpms
  - rhel-7-server-rpms
  - rhel-7-server-satellite-tools-6.2-rpms
  - rhel-7-server-openstack-10-tools-rpms
  - rhel-7-server-rhscon-2-installer-rpms
  - rhel-7-server-rhceph-2-mon-rpms
  - rhel-7-server-rhceph-2-osd-rpms


yum_gpgcheck: no
yum_gpgcheck_key: "file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release"

# satellite URL to be used
satellite_url: https://satellite6.example.com/pulp/repos

# Satellite organisation to be used
satellite_org: MyOrganization

# RPM with the Satellite keys
satellite_pkg: https://satellite6.example.com/pub/katello-ca-consumer-latest.noarch.rpm

# Timeout used when creating instances
instance_creation_timeout: 200

# this can be used to nominate nodes for running os-on-os instances. Set to "nova" if not required
instance_az: "nova"


git_repo_server: gitlab.consulting.redhat.com

git_repos:
  foreman: "ssh://git@gitlocation.server.example.com/Group/foreman.git"
  icloud: "ssh://git@gitlocation.server.example:2222/Group/icloud-puppet.git"
  ceph_ansible:
    url: "ssh://git@gitlocation.server.example:2222/Group/ceph-ansible.git"
    branch: "stable-2.2"
  os_inception:
    url: "ssh://git@gitlocation.server.example:2222/Group/os-inception.git"
    branch: "allinone"


r10k_environment: icloud_{{ icloud_puppet_branch }}

use_existing_external_network: "false"
use_existing_ceph: "false"

# End required configuration

# The below is used to ensure the instances span multiple nodes for HA (pacemaker/loadbalanced services)
affinity_policy: "anti-affinity"
use_affinity_groups: "true"

# Internal configuration
# These values are considered "internal". Changes to this file are not required to get
# the playbook to work, but can be made by power-users

# The project user and role to be used by os-on-os. This is created in the admin playbook
# let's prefix everything with the project name, so that we can have multiple environments
prefix: "{{ os_project }}"

# Images which will be uploaded to the "host" cloud (running virtual cloud instances).
# Files need to be provided with the playbook
images:
  pxeboot:
    filename: pxeboot.img
    name: "{{ prefix }}_pxeboot"
    disk_format: raw
    container_format: bare

remote_images:
  rhel-server-7.5-x86_64:
    url: http://repos.example.com/images/rhel-server-7.5-update-3-x86_64-kvm.qcow2
    filename: rhel-server-7.5-x86_64-kvm.qcow2
    name: "{{ os_project }}-rhel-server-7.5-x86_64-kvm"
    disk_format: qcow2
    container_format: bare
# this is commented out as we are not shipping rhel-7.3-35.qcow2 with the playbook.
# if the file is shipped with the code in the appropriate task's "files" directory
# this can be uncommented
#  rhel:
#    filename: rhel-7.qcow2
#    name: "{{ prefix }}_rhel-7"
#    disk_format: qcow2
#    container_format: bare

# Quota to be set for the OS-on-OS project by the playbook if create_project is true and
# valid admin creds are provided
quota:
  ram: 200000
  vcpus: 50
  instances: 20
  ports: 250

overcloud_rootpw: passw0rd

# a simple list of node classes, used for with_items loops in the tasks
node_types:
 - bastion
 - pacemaker
 - service
 - ceph
 - compute
 - ospd

preprovisioned_nodes: false

# Flavors to be created by the plabook if create_flavors is true and valid admin creds are
# provided
flavors:
  bastion:
    name: "{{ prefix }}.bastion"
    vcpus: 2
    ram: 8192
    disk: 20
    ephemeral: 0
    extra_specs: {}
  pacemaker:
    name: "{{ prefix }}.pacemaker"
    vcpus: 2
    ram: 8192
    disk: 40
    ephemeral: 0
    extra_specs: {}
  service:
    name: "{{ prefix }}.service"
    vcpus: 2
    ram: 16384
    disk: 40
    ephemeral: 0
    extra_specs: {}
  compute:
    name: "{{ prefix }}.compute"
    vcpus: 4
    ram: 6144
    disk: 40
    ephemeral: 0
    extra_specs: {}
  ceph:
    name: "{{ prefix }}.ceph"
    vcpus: 2
    ram: 4096
    disk: 20
    ephemeral: 0
    extra_specs: {}
  ospd:
    name: "{{ prefix }}.ospd"
    vcpus: 8
    ram: 16384
    disk: 80
    ephemeral: 0
    extra_specs: {}

flavors_nested:
  pacemaker:
    name: "{{ prefix }}.pacemaker"
    vcpus: 2
    ram: 8192
    disk: 20
    ephemeral: 0
    extra_specs: {}
  service:
    name: "{{ prefix }}.service"
    vcpus: 2
    ram: 16384
    disk: 10
    ephemeral: 0
    extra_specs: {}
  compute:
    name: "{{ prefix }}.compute"
    vcpus: 4
    ram: 6144
    disk: 10
    ephemeral: 0
    extra_specs:
      aggregate_instance_extra_specs:
        filter_tenant_id: "{{ os_project_id }}"
  ceph:
    name: "{{ prefix }}.ceph"
    vcpus: 2
    ram: 4096
    disk: 20
    ephemeral: 0
    extra_specs: {}
  ospd:
    name: "{{ prefix }}.ospd"
    vcpus: 4
    ram: 16384
    disk: 80
    ephemeral: 0
    extra_specs: {}

# BUG WARNING: using custom security groups with multiple interfaces, some of which are on port_security_disabled networks
#  can result in boot ERRORS. For this reason, we only use the default security group which seems to work somehow.
os_inception_secgroup: default


# node definition, used by the instance creation task
# node counts, connected networks, flavor, image and secgroups
# can be customised here
nodes:
  bastion:
    flavor: "{{ prefix }}.bastion"
    image: "{{ remote_images['rhel-server-7.5-x86_64'].name }}"
    secgroup: "{{ os_inception_secgroup }}"
    count: 1
    auto_ip: no
    wait: true
    networks:
      - provisioning
      - storage
      - internalapi
      - external
  pacemaker:
    flavor: "{{ prefix }}.pacemaker"
    image: "{{ prefix }}_pxeboot"
    secgroup: "{{ os_inception_secgroup }}"
    count: 3
    auto_ip: no
    wait: false
    networks:
      - provisioning
      - storage
      - storagemgmt
      - internalapi
      - external
  service:
    flavor: "{{ prefix }}.service"
    image: "{{ prefix }}_pxeboot"
    secgroup: "{{ os_inception_secgroup }}"
    count: 3
    auto_ip: no
    wait: false
    networks:
      - provisioning
      - storage
      - storagemgmt
      - internalapi
      - tenant
      - external
  ceph:
    flavor: "{{ prefix }}.ceph"
    image: "{{ remote_images['rhel-server-7.5-x86_64'].name }}"
    secgroup: "{{ os_inception_secgroup }}"
    count: 3
    auto_ip: no
    wait: false
    networks:
      - external
      - storage
      - storagemgmt
    volumes: 2
    volsize: 10
  compute:
    flavor: "{{ prefix }}.compute"
    image: "{{ prefix }}_pxeboot"
    secgroup: "{{ os_inception_secgroup }}"
    count: 2
    auto_ip: no
    wait: false
    networks:
      - provisioning
      - storage
      - internalapi
      - tenant
      - external
  ospd:
    flavor: "{{ prefix }}.ospd"
    image: "rhel-server-7.4-x86_64-kvm.qcow2-20170801"
    secgroup: "{{ os_inception_secgroup }}"
    count: 1
    auto_ip: no
    wait: true
    networks:
      - provisioning
      - external

subnets_allocation_pool_start: 30
subnets_allocation_pool_end: 250

# network definitions used for network, subnet and port creation
# can be customised as required. These are the default OSPd values
networks:
  provisioning:
    name: "{{ prefix }}_provisioning"
    cidr: "192.0.2.0/24"
    netmask_length: 24
    cidr_prefix: "192.0.2"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: false
    no_gateway_ip: true
    gateway_ip:
    dns_nameservers: []
    pool_start: 10
    pool_end:  250
    use_existing: "false"
  internalapi:
    name: "{{ prefix }}_internalapi"
    cidr: "172.21.0.0/24"
    cidr_prefix: "172.21.0"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: true
    no_gateway_ip: true
    gateway_ip:
    dns_nameservers: []
    pool_start: 10
    pool_end:  250
    use_existing: "false"
  storage:
    name: "{{ prefix }}_storage"
    cidr: "172.22.0.0/24"
    cidr_prefix: "172.22.0"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: true
    no_gateway_ip: true
    gateway_ip:
    dns_nameservers: []
    pool_start: 10
    pool_end:  250
    use_existing: "false"
  storagemgmt:
    name: "{{ prefix }}_storagemgmt"
    cidr: "172.19.0.0/24"
    cidr_prefix: "172.19.0"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: true
    no_gateway_ip: true
    gateway_ip:
    dns_nameservers: []
    pool_start: 10
    pool_end:  250
    use_existing: "false"
  tenant:
    name: "{{ prefix }}_tenant"
    cidr: "172.16.0.0/24"
    cidr_prefix: "172.16.0"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: true
    no_gateway_ip: true
    gateway_ip:
    dns_nameservers: []
    pool_start: 10
    pool_end:  250
    use_existing: "false"
  external:
    name: "{{ prefix }}_external"
    cidr: "10.0.0.0/24"
    cidr_prefix: "10.0.0"
    port_security_enabled: false
    no_security_groups: true
    enable_dhcp: true
    no_gateway_ip: false
    gateway_ip: 10.0.0.1
    dns_nameservers: ["10.76.23.25"]
    pool_start: 10
    pool_end: 250
    use_existing: "false"

# last octets used for configuring fixed_ips on ports belonging
# to instances. Last octets are consistent in each network
last_octet:
  bastion:
    bastion_1: 252
  ospd:
    ospd_1: 253
  pacemaker:
    pacemaker_1: 31
    pacemaker_2: 32
    pacemaker_3: 33
  service:
    service_1: 14
    service_2: 15
    service_3: 16
  ceph:
    ceph_1: 17
    ceph_2: 18
    ceph_3: 19
  compute:
    compute_1: 20
    compute_2: 21




# this controls whether yum update will be run on the OSPd or not
ospd_install_updates: true

# packages required by the undercloud - plus some useful tools
undercloud_packages:
  - python-tripleoclient
  - openstack-utils
  - vim
  - strace
  - sysstat
  - screen
  - rhosp-director-images-ipa
  - rhosp-director-images

# undercloud definition - used for generating undercloud.conf
# this is based on OSPd defaults
undercloud:
  local_ip: "{{ networks.provisioning.cidr_prefix }}.1/{{ networks.provisioning.netmask_length }}"
  network_gateway: "{{ networks.provisioning.cidr_prefix }}.1"
  undercloud_public_vip: "{{ networks.provisioning.cidr_prefix }}.2"
  undercloud_admin_vip: "{{ networks.provisioning.cidr_prefix }}.3"
  local_interface: eth1
  local_mtu: 1500
  network_cidr: "{{ networks.provisioning.cidr }}"
  masquerade_network: "{{ networks.provisioning.cidr }}"
  dhcp_start: "{{ networks.provisioning.cidr_prefix }}.5"
  dhcp_end: "{{ networks.provisioning.cidr_prefix }}.24"
  undercloud_admin_password: "{{ undercloud_admin_password }}"

# undercloud environment variables used for running openstack commands
# on the undercloud using shell module
undercloud_auth_env_v2:
  NOVA_VERSION: 1.1
  OS_PASSWORD: "{{ undercloud_admin_password }}"
  OS_AUTH_URL: "https://{{ undercloud.undercloud_public_vip }}:13000/v2.0"
  OS_USERNAME: admin
  OS_TENANT_NAME: admin
  COMPUTE_API_VERSION: 1.1
  OS_BAREMETAL_API_VERSION: 1.15
  OS_NO_CACHE: True
  OS_CLOUDNAME: undercloud
  OS_IMAGE_API_VERSION: 1
  OS_CACERT: /etc/pki/ca-trust/source/anchors/cm-local-ca.pem

undercloud_auth_env_v3:
  NOVA_VERSION: 1.1
  OS_PASSWORD: "{{ undercloud_admin_password }}"
  OS_AUTH_URL: "https://{{ undercloud.undercloud_public_vip }}:13000/"
  OS_USERNAME: admin
  OS_AUTH_TYPE: password
  OS_PROJECT_NAME: admin
  OS_USER_DOMAIN_NAME: Default
  OS_PROJECT_DOMAIN_NAME: Default
  COMPUTE_API_VERSION: 1.1
  OS_BAREMETAL_API_VERSION: 1.34
  OS_NO_CACHE: True
  OS_CLOUDNAME: undercloud
  OS_IDENTITY_API_VERSION: 3
  OS_CACERT: /etc/pki/ca-trust/source/anchors/cm-local-ca.pem

# overcloud image files which will be unpacked and uploaded to undercloud glance
overcloud_image_files:
  - /usr/share/rhosp-director-images/overcloud-full-latest-{{ osp_version }}.0.tar
  - /usr/share/rhosp-director-images/ironic-python-agent-latest-{{ osp_version }}.0.tar

# yum packages which will be installed on the ansible node
yum_prerequisites:
  - git
  - patch
  - gcc
  - make
  - rubygems
  - rubygem-bundler
  - qemu-img
  - jq
  - vim
  - python-devel
  - sshpass


getpip_url: https://bootstrap.pypa.io/get-pip.py


virtualenv_name: openstacksdk2

pip_module_url: http://repos.gpslab.cbr.redhat.com/modules/

# pip packages which will be installed on the ansible node
old_pip_prerequisites:
  - yq
  - python-openstackclient
  - shade==1.26
  - ansible==2.5.3

pip_prerequisites:
  - python-openstackclient
  - python-neutronclient
  - openstacksdk==0.15
  - ansible==2.6.4
  - stevedore==1.27
  - requests==2.18.0
  - keystoneauth1==3.9.0
  - ipaddress==1.0.17


pip_conflicting_rpms:
  - pyOpenSSL
  - PyYAML
  - python-requests
  - python-ipaddress
  - python-inotify


# Install gems from a network url or use the local versions.
gem_install: true

# gem source url of the form http://<gem repo server>/artifactory/api/gems/gems/
gemsource_url: https://rubygems.org/

# misc internal settings. These shouldn't need to be changed

port_wait: false
yum_timeout: 1800
yum_poll: 5
stack_user: stack

# End internal configuration
